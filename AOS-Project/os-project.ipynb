{"cells":[{"cell_type":"markdown","id":"04053483","metadata":{},"source":["Bi-directional using Keras"]},{"cell_type":"code","execution_count":null,"id":"5fa07300","metadata":{},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","def load_glove_embeddings(embeddings_path):\n","    embeddings_index = {}\n","    with open(embeddings_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","    return embeddings_index\n","\n","def create_embedding_matrix(word_index, embeddings_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","    new_word_index = {}\n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","            new_word_index[word] = i\n","    return embedding_matrix, new_word_index, vocab_size\n","\n","\n","\n","def read_data(normal_logs, abnormal_logs):\n","    data, labels = [], []\n","    for filepath in normal_logs:\n","        with open(filepath) as fp:\n","            for line in fp:\n","                content = line.strip()\n","                data.append(content)\n","                labels.append(0)\n","    for filepath in abnormal_logs:\n","        with open(filepath) as fp:\n","            for line in fp:\n","                content = line.strip()\n","                data.append(content)\n","                labels.append(1)\n","    return data, np.array(labels)\n","\n","# Define paths\n","normal_train_paths = [\"/home/aks/AOS-Project/filter/ceph-dout-filter.txt\", \"/home/aks/AOS-Project/filter/glustrefs_info-filter.txt\", \"/home/aks/AOS-Project/filter/daos-debug-filter.txt\",\"/home/aks/AOS-Project/filter/orangefs-debug-filter.txt\",\"/home/aks/AOS-Project/filter/beegfs-debug-filter.txt\",\"/home/aks/AOS-Project/filter/hive-debug-filter.txt\"]\n","abnormal_train_paths = [\"/home/aks/AOS-Project/filter/ceph-derr-filter.txt\", \"/home/aks/AOS-Project/filter/glustrefs_error-filter.txt\", \"/home/aks/AOS-Project/filter/daos-error-filter.txt\", \"/home/aks/AOS-Project/filter/orangefs-error-filter.txt\",\"/home/aks/AOS-Project/filter/beegfs-error-filter.txt\",\"/home/aks/AOS-Project/filter/hive-error-filter.txt\"]\n","normal_test_paths = [\"/home/aks/AOS-Project/filter/lustre-debug-filter.txt\",\"/home/aks/AOS-Project/filter/hbase-debug-filter.txt\",\"/home/aks/AOS-Project/filter/hdfs-debug-filter.txt\"]\n","abnormal_test_paths = [\"/home/aks/AOS-Project/filter/lustre-error-filter.txt\",\"/home/aks/AOS-Project/filter/hbase-error-filter.txt\",\"/home/aks/AOS-Project/filter/hdfs-error-filter.txt\"]\n","glove_path = '/home/aks/DRILL/glove-embeddings/glove.6B.100d.txt'\n","\n","\n","train_data, train_labels = read_data(normal_train_paths, abnormal_train_paths)\n","test_data, test_labels = read_data(normal_test_paths, abnormal_test_paths)\n","\n","# Tokenization\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_data + test_data)\n","\n","# Load GloVe embeddings\n","embeddings_index = load_glove_embeddings(glove_path)\n","embedding_dim = 100  \n","embedding_matrix, new_word_index, vocab_size = create_embedding_matrix(tokenizer.word_index, embeddings_index, embedding_dim)\n","\n","# Adjust tokenizer's word index to match the new vocabulary\n","tokenizer.word_index = new_word_index\n","tokenizer.num_words = vocab_size\n","\n","# Prepare data\n","train_sequences = tokenizer.texts_to_sequences(train_data)\n","test_sequences = tokenizer.texts_to_sequences(test_data)\n","max_len = 100\n","train_padded = pad_sequences(train_sequences, maxlen=max_len)\n","test_padded = pad_sequences(test_sequences, maxlen=max_len)\n","\n","# Define the model\n","model = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, weights=[embedding_matrix[:vocab_size]], trainable=False),\n","    Bidirectional(LSTM(100, return_sequences=True)),\n","    Bidirectional(LSTM(100)),\n","    Dense(400, activation='relu'),\n","    Dense(2, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","# Train the model\n","history = model.fit(train_padded, train_labels, epochs=5, validation_data=(test_padded, test_labels))\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_padded, test_labels)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_acc)\n","\n","# Function to evaluate the model using precision, recall, and F1 score\n","def evaluate_model(model, test_data, test_labels):\n","    predictions = model.predict(test_data)\n","    predicted_labels = np.argmax(predictions, axis=1)\n","    precision = precision_score(test_labels, predicted_labels)\n","    recall = recall_score(test_labels, predicted_labels)\n","    f1 = f1_score(test_labels, predicted_labels)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","\n","# Evaluate the model using precision, recall, and F1 score\n","evaluate_model(model, test_padded, test_labels)\n"]},{"cell_type":"code","execution_count":null,"id":"ac6db8b9","metadata":{},"outputs":[],"source":["model.save('my_model_bi_RNN.h5')"]},{"cell_type":"code","execution_count":null,"id":"61d238e9","metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import accuracy_score\n","import seaborn as sns\n","\n","sns.set(style=\"whitegrid\")\n","\n","# Load the saved model\n","model = load_model('my_model_bi_RNN.h5')\n","\n","# Assume test_data_files is a list of paths to the test data files\n","lustre_normal_test_paths = [\"/home/aks/DRILL/sentilog/filter/lustre-debug-filter.txt\"]\n","hdfs_normal_test_paths = [\"/home/aks/DRILL/sentilog/filter/hdfs-debug-filter.txt\"]\n","hbase_normal_test_paths = [\"/home/aks/DRILL/sentilog/filter/hbase-debug-filter.txt\"]\n","\n","lustre_abnormal_test_paths = [\"/home/aks/DRILL/sentilog/filter/lustre-error-filter.txt\"]\n","hdfs_abnormal_test_paths = [\"/home/aks/DRILL/sentilog/filter/hbase-error-filter.txt\"]\n","hbase_abnormal_test_paths = [\"/home/aks/DRILL/sentilog/filter/hdfs-error-filter.txt\"]\n","\n","\n","lustre_test_data, lustre_test_labels = read_data(lustre_normal_test_paths, lustre_abnormal_test_paths)\n","hdfs_test_data, hdfs_test_labels = read_data(hdfs_normal_test_paths, hdfs_abnormal_test_paths)\n","hbase_test_data, hbase_test_labels = read_data(hbase_normal_test_paths, hbase_abnormal_test_paths)\n","\n","lustre_test_sequences = tokenizer.texts_to_sequences(lustre_test_data)\n","hdfs_test_sequences = tokenizer.texts_to_sequences(hdfs_test_data)\n","hbase_test_sequences = tokenizer.texts_to_sequences(hbase_test_data)\n","\n","max_len = 100\n","\n","lustre_test_padded = pad_sequences(lustre_test_sequences, maxlen=max_len)\n","hdfs_test_padded = pad_sequences(hdfs_test_sequences, maxlen=max_len)\n","hbase_test_padded = pad_sequences(hbase_test_sequences, maxlen=max_len)\n","\n","lustre_test_loss, lustre_test_acc = model.evaluate(lustre_test_padded, lustre_test_labels)\n","print(\"Lustre Test Loss:\", lustre_test_loss)\n","print(\"Lustre Test Accuracy:\", lustre_test_acc)\n","\n","hdfs_test_loss, hdfs_test_acc = model.evaluate(hdfs_test_padded, hdfs_test_labels)\n","print(\"Hdfs Test Loss:\", hdfs_test_loss)\n","print(\"Hdfs Test Accuracy:\", hdfs_test_acc)\n","\n","hbase_test_loss, hbase_test_acc = model.evaluate(hbase_test_padded, hbase_test_labels)\n","print(\"Hbase Test Loss:\", hbase_test_loss)\n","print(\"Hbase Test Accuracy:\", hbase_test_acc)\n","\n","dataset_accuracies = {}\n","\n","dataset_accuracies[\"Lustre\"] = lustre_test_acc\n","dataset_accuracies[\"Hdfs\"] = hdfs_test_acc\n","dataset_accuracies[\"Hbase\"] = hbase_test_acc\n","\n","# Plotting\n","datasets = list(dataset_accuracies.keys())\n","accuracies = list(dataset_accuracies.values())\n","plt.figure(figsize=(12, 6)) \n","plt.bar(datasets, accuracies, color='skyblue', width=0.4)  \n","plt.xlabel('Dataset')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy vs Dataset using Bi_Directional Deep Learning Model')\n","plt.xticks(rotation=45, ha=\"right\") \n","plt.tight_layout()  \n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4624973,"sourceId":7880165,"sourceType":"datasetVersion"},{"datasetId":4812306,"sourceId":8139684,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":5}
